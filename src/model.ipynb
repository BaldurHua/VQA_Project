{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loaded: 443757 samples\n",
      "Validation Data Loaded: 214354 samples\n",
      "Test Data Loaded: 447793 samples\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed/preprocessed_train.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed/preprocessed_val.pkl\", \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "\n",
    "with open(\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed/preprocessed_test.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "print(f\"Training Data Loaded: {len(train_data)} samples\")\n",
    "print(f\"Validation Data Loaded: {len(val_data)} samples\")\n",
    "print(f\"Test Data Loaded: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens\n",
    "\n",
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "class VocabDict:\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        self.word_list = load_str_list(vocab_file)\n",
    "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        self.unk2idx = self.word2idx_dict[''] if '' in self.word2idx_dict else None\n",
    "\n",
    "    def idx2word(self, n_w):\n",
    "\n",
    "        return self.word_list[n_w]\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        if w in self.word2idx_dict:\n",
    "            return self.word2idx_dict[w]\n",
    "        elif self.unk2idx is not None:\n",
    "            return self.unk2idx\n",
    "        else:\n",
    "            return self.word2idx_dict.get(\"<unk>\", 0) \n",
    "\n",
    "    def tokenize_and_index(self, sentence):\n",
    "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
    "\n",
    "        return inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaDataset(Dataset):\n",
    "    def __init__(self, input_dir, input_vqa, max_qst_length=30, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.vqa = np.load(f\"{input_dir}/{input_vqa}\", allow_pickle=True)\n",
    "        self.qst_vocab = VocabDict(f\"{input_dir}/vocab_questions.txt\")\n",
    "        self.ans_vocab = VocabDict(f\"{input_dir}/vocab_answers.txt\")\n",
    "        self.max_qst_length = max_qst_length\n",
    "        self.transform = transform\n",
    "        self.load_ans = \"answer_label\" in self.vqa[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vqa = self.vqa[idx]\n",
    "\n",
    "        # Load Image\n",
    "        image = cv2.imread(vqa[\"image_path\"])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process Question \n",
    "        qst_tokens = vqa[\"question_tokens\"]\n",
    "        qst2idc = np.full(self.max_qst_length, self.qst_vocab.word2idx(\"<unk>\"))\n",
    "        qst2idc[:len(qst_tokens)] = qst_tokens[:self.max_qst_length]\n",
    "\n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"question\": torch.tensor(qst2idc, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        # Process Answers\n",
    "        if self.load_ans:\n",
    "            sample[\"answer_label\"] = torch.tensor(vqa[\"answer_label\"], dtype=torch.long)\n",
    "\n",
    "            # Multi-choice \n",
    "            MAX_ANSWERS = 10\n",
    "            answer_multi_choice = vqa.get(\"valid_answers\", [])\n",
    "            \n",
    "            if len(answer_multi_choice) < MAX_ANSWERS:\n",
    "                answer_multi_choice += [0] * (MAX_ANSWERS - len(answer_multi_choice))\n",
    "            else:\n",
    "                answer_multi_choice = answer_multi_choice[:MAX_ANSWERS]\n",
    "\n",
    "            sample[\"answer_multi_choice\"] = torch.tensor(answer_multi_choice, dtype=torch.long)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vqa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 30])\n",
      "tensor([  5,   1,  10,   6,   3,  21, 414, 306,   2,  30,   1,   5,   2, 353,\n",
      "         13,   1,   0,   4,  54, 924,   2,   2,   1,   1, 138,   5,   0,  27,\n",
      "         90, 290,   1,   1])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()  \n",
    "])\n",
    "\n",
    "train_dataset = VqaDataset(input_dir=\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed\", \n",
    "                           input_vqa=\"preprocessed_train.pkl\", \n",
    "                           transform=transform)\n",
    "\n",
    "val_dataset = VqaDataset(input_dir=\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed\", \n",
    "                           input_vqa=\"preprocessed_val.pkl\", \n",
    "                           transform=transform)\n",
    "\n",
    "test_dataset = VqaDataset(input_dir=\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed\", \n",
    "                           input_vqa=\"preprocessed_test.pkl\", \n",
    "                           transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(sample_batch[\"image\"].shape) \n",
    "print(sample_batch[\"question\"].shape) \n",
    "print(sample_batch.get(\"answer_label\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        out_channels_branch = 16 \n",
    "        self.branch1x1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(out_channels_branch, out_channels_branch, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, out_channels_branch, kernel_size=1)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1) \n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResIncepEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ResIncepEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Residual Blocks\n",
    "        self.res_block1 = ResBlock(32)\n",
    "        self.res_block2 = ResBlock(32)\n",
    "\n",
    "        # Inception module\n",
    "        self.inception = InceptionModule(32)\n",
    "\n",
    "        # Double Convolution Layers\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Feature Embedding Layer\n",
    "        self.fc_embed = nn.Linear(128, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.inception(x)\n",
    "\n",
    "        x = self.relu2_1(self.bn2_1(self.conv2_1(x)))\n",
    "        x = self.relu2_2(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.relu3_1(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.relu3_2(self.bn3_2(self.conv3_2(x)))\n",
    "\n",
    "        x = self.pool2(x)  \n",
    "\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, channels, height * width).transpose(1, 2)  \n",
    "\n",
    "        x = self.fc_embed(x)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers=2, hidden_size=256):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
    "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * num_layers * hidden_size, embed_size) \n",
    "        self.norm = nn.LayerNorm(embed_size) \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.word2vec(question)  \n",
    "        qst_vec, (hidden, _) = self.lstm(qst_vec)  \n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2).contiguous().view(hidden.size(1), -1)  \n",
    "\n",
    "        qst_feature = self.tanh(self.fc(hidden))  \n",
    "        qst_feature = self.norm(qst_feature) \n",
    "        return qst_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_channels, embed_size, dropout=True):\n",
    "        super(Attention, self).__init__()\n",
    "        self.ff_image = nn.Linear(embed_size, num_channels)\n",
    "        self.ff_questions = nn.Linear(embed_size, num_channels)\n",
    "        self.dropout = nn.Dropout(p=0.5) if dropout else nn.Identity()\n",
    "        self.ff_attention = nn.Linear(num_channels, 1)\n",
    "        self.norm = nn.LayerNorm(embed_size)  \n",
    "\n",
    "    def forward(self, vi, vq):\n",
    "        hi = self.ff_image(vi)  \n",
    "        hq = self.ff_questions(vq).unsqueeze(dim=1)  \n",
    "        ha = torch.tanh(hi + hq)\n",
    "\n",
    "        ha = self.dropout(ha)  \n",
    "        ha = self.ff_attention(ha) \n",
    "\n",
    "        pi = torch.softmax(ha, dim=1)  \n",
    "        vi_attended = (pi * vi).sum(dim=1)\n",
    "\n",
    "        u = self.norm(vi_attended + vq) \n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SANModel(nn.Module):\n",
    "    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size):\n",
    "        super(SANModel, self).__init__()\n",
    "        self.num_attention_layer = 2\n",
    "\n",
    "        # Encoders\n",
    "        self.img_encoder = ResIncepEncoder(embed_size)\n",
    "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
    "\n",
    "        # Attention Layers\n",
    "        self.san = nn.ModuleList([Attention(embed_size, embed_size) for _ in range(self.num_attention_layer)])\n",
    "\n",
    "        # MLP \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, ans_vocab_size) \n",
    "        )\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        # Encode Image & Question\n",
    "        img_feature = self.img_encoder(img)  \n",
    "        qst_feature = self.qst_encoder(qst) \n",
    "\n",
    "        # Apply Stacked Attention\n",
    "        u = qst_feature\n",
    "        for attn_layer in self.san:\n",
    "            u = attn_layer(img_feature, u)\n",
    "\n",
    "        combined_feature = self.mlp(u)  \n",
    "        return combined_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "train_dataset = VqaDataset(input_dir=\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed\", \n",
    "                           input_vqa=\"preprocessed_train.pkl\", \n",
    "                           transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataset = VqaDataset(input_dir=\"C:/Users/Baldu/Desktop/Temp/VQA/data/preprocessed\", \n",
    "                       input_vqa=\"preprocessed_val.pkl\", \n",
    "                       transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "model_dir = 'C:/Users/Baldu/Desktop/Temp/VQA/outputs'\n",
    "log_dir = 'C:/Users/Baldu/Desktop/Temp/VQA/outputs'\n",
    "\n",
    "train_dataset = train_loader.dataset\n",
    "qst_vocab_size = train_dataset.qst_vocab.vocab_size\n",
    "ans_vocab_size = train_dataset.ans_vocab.vocab_size\n",
    "ans_unk_idx = train_dataset.ans_vocab.unk2idx\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SANModel(\n",
    "        embed_size=1024,\n",
    "        qst_vocab_size=qst_vocab_size,\n",
    "        ans_vocab_size=ans_vocab_size,\n",
    "        word_embed_size=300,\n",
    "        num_layers=2,\n",
    "        hidden_size=64)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.1, patience=10)\n",
    "\n",
    "last_time = 0\n",
    "early_stop_threshold = 3\n",
    "best_loss = 99999\n",
    "val_increase_count = 0\n",
    "stop_training = False\n",
    "prev_loss = 9999\n",
    "num_epochs = 10\n",
    "save_step = 1\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if stop_training:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    for phase in ['train', 'valid']: \n",
    "        running_loss = 0.0\n",
    "        running_corr = 0\n",
    "        batch_step_size = len(train_loader) if phase == 'train' else len(val_loader) \n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        last_time = time.time()\n",
    "\n",
    "        for batch_idx, batch_sample in enumerate(train_loader if phase == 'train' else val_loader): \n",
    "            image = batch_sample['image'].to(device)\n",
    "            question = batch_sample['question'].to(device)\n",
    "            label = batch_sample['answer_label'].to(device)\n",
    "            multi_choice = batch_sample['answer_multi_choice']\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'): \n",
    "                output = model(image, question)\n",
    "                _, pred = torch.max(output, 1)\n",
    "                loss = criterion(output, label)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            if len(multi_choice) == 0:\n",
    "                multi_choice = torch.zeros((1, pred.shape[0]), dtype=torch.long).to(device)  \n",
    "            else:\n",
    "                multi_choice = torch.tensor(multi_choice, dtype=torch.long).to(device)\n",
    "            if multi_choice.dim() == 1:\n",
    "                multi_choice = multi_choice.unsqueeze(0)\n",
    "            running_corr += (multi_choice == pred.unsqueeze(1)).any(dim=1).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                time_taken = time.time() - last_time\n",
    "                time_left = (((batch_step_size - batch_idx) * time_taken) / 100) * (num_epochs - epoch)\n",
    "                print(f'| {phase.upper()} SET | Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{batch_step_size}], '\n",
    "                      f'Loss: {loss.item():.4f}, Time left: {time_left / 3600:.2f} hr')\n",
    "                last_time = time.time()\n",
    "        epoch_loss = running_loss / batch_step_size\n",
    "        epoch_acc = running_corr / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
    "        print(f'| {phase.upper()} SET | Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\\n')\n",
    "        log_file = os.path.join(log_dir, f'{phase}-log.txt')\n",
    "\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f'Epoch {epoch+1}\\tLoss: {epoch_loss:.4f}\\tAcc: {epoch_acc:.4f}\\n')\n",
    "        if phase == 'valid':\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), os.path.join(model_dir, 'best_model.pt'))\n",
    "            if epoch_loss > prev_loss:\n",
    "                val_increase_count += 1\n",
    "            else:\n",
    "                val_increase_count = 0\n",
    "            if val_increase_count >= early_stop_threshold:\n",
    "                stop_training = True\n",
    "            prev_loss = epoch_loss\n",
    "\n",
    "    if (epoch+1) % save_step == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, f'model-epoch-{epoch+1:02d}.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
